# -*- coding: utf-8 -*-
"""DLassignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SmN-GgImHFyugLDDz3NzVuJBuxY4xhbq

Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?

Yes, it is generally acceptable to initialize all weights to the same value as long as that value is randomly selected using He initialization. He initialization helps address the vanishing/exploding gradient problem in deep neural networks. In Python, you can implement He initialization using the NumPy library by calculating the recommended standard deviation and initializing weights with random values from a normal distribution. This ensures that each neuron receives a different initial value, enabling effective training of the network.

Is it OK to initialize the bias terms to 0?

Yes, it is generally acceptable to initialize the bias terms to 0 in neural networks. Initializing biases to 0 simplifies the initial predictions and allows the network to learn appropriate bias values during training. In Python, you can initialize the bias terms to 0 by creating an array of zeros with the desired output size.

Name three advantages of the SELU activation function over ReLU.

Self-normalization: SELU preserves the mean and standard deviation of the input distribution within each layer, enabling automatic normalization and improving training stability.

Avoids dead neurons: SELU allows negative values, preventing the complete "kill" of neurons and ensuring that all neurons contribute to the learning process.

Better representation power: SELU captures both positive and negative signals effectively with a non-zero derivative for both, providing a more balanced representation power compared to ReLU.

SELU: Use in deep neural networks with many layers to promote self-normalization and stabilize training.

Leaky ReLU and variants: Use to address the dying ReLU problem and allow small gradients for negative values, promoting learning.

ReLU: Use as a default choice for most hidden layers in deep neural networks due to its simplicity and computational efficiency.

tanh: Use when you need an activation function that outputs values between -1 and 1, often used in RNNs and certain autoencoders.

logistic (sigmoid): Use in binary classification problems or as the final activation function in multi-class classification with logistic regression.

softmax: Use as the final activation function in multi-class

What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?

Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer can lead to overshooting, instability, slow convergence, and difficulty in escaping local minima. This can result in poor optimization performance and hinder the training of the model.

Name three ways you can produce a sparse model

L1 Regularization: Use L1 regularization (Lasso regularization) to encourage the model to set many weights to zero, resulting in a sparse model. Libraries like scikit-learn provide L1 regularization through their linear models, such as Lasso regression.

Feature Selection: Apply feature selection techniques to identify and select the most relevant features for the model. Techniques like univariate feature selection, recursive feature elimination, or feature importance ranking can help achieve sparsity by considering only the important features.

Pruning: Train a dense model and then prune it by removing connections or setting small weights to zero. Pruning techniques can be applied iteratively or based on a threshold. Libraries like TensorFlow provide pruning functionality to selectively remove weights and create a sparse model.

Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?

Dropout can slow down training because it introduces randomness and reduces the capacity of the network, requiring longer training to achieve similar performance as a model without dropout.
Dropout does not slow down inference because it is typically turned off during inference, where predictions are made on new instances.
MC Dropout, which applies dropout during inference by running multiple forward passes, can slow down the inference process compared to traditional inference without dropout. However, it provides more robust predictions by capturing model uncertainty.

Practice training a deep neural network on the CIFAR10 image dataset: Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed? Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.). Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import os

keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(keras.layers.Dense(100,
                                 activation="elu",
                                 kernel_initializer="he_normal"))

model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(learning_rate=5e-5)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()

x_train = X_train[5000:]
y_train = y_train[5000:]
x_test = X_test[:5000]
y_test = y_test[:5000]

early_stopping = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint =keras.callbacks.ModelCheckpoint("my_cifar10_model.h8", save_best_only= True)

run_index = 1
run_logdir = os.path.join(os.curdir, "my_cid=far10_logs", "rum_{:03d}".format(run_index))
tensorboard = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping, model_checkpoint, tensorboard]

model.fit(x_train, y_train, epochs= 100,
          validation_data=(x_test, y_test),
          callbacks= callbacks)

model = keras.models.load_model("my_cifar10_model.h8")
model.evaluate(x_test, y_test)

import tensorflow as tf
from tensorflow import keras
import numpy as np

keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(keras.layers.Dense(100, kernel_initializer="lecun_normal", activation="selu"))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(learning_rate=5e-4)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint("my_cifar10_selu_model.h5", save_best_only=True)
run_index = 1
run_logdir = os.path.join(os.curdir, "my_cifar10_logs", "run_selu_{:03d}".format(run_index))
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()

X_train = X_train_full[5000:]
y_train = y_train_full[5000:]
X_valid = X_train_full[:5000]
y_valid = y_train_full[:5000]

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

model.fit(X_train_scaled, y_train, epochs=100,
          validation_data=(X_valid_scaled, y_valid),
          callbacks=callbacks)

model = keras.models.load_model("my_cifar10_selu_model.h5")
model.evaluate(X_valid_scaled, y_valid)

model.add(keras.layers.AlphaDropout(rate=0.1))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(learning_rate=5e-4)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint("my_cifar10_alpha_dropout_model.h5", save_best_only=True)
run_index = 1 
run_logdir = os.path.join(os.curdir, "my_cifar10_logs", "run_alpha_dropout_{:03d}".format(run_index))
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

model.fit(X_train_scaled, y_train, epochs=100,
          validation_data=(X_valid_scaled, y_valid),
          callbacks=callbacks)

model = keras.models.load_model("my_cifar10_alpha_dropout_model.h5")
model.evaluate(X_valid_scaled, y_valid)